{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f33224c-6c18-4bbc-a8ce-cec1277c4bcc",
   "metadata": {},
   "source": [
    "## Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2b1c6-8f27-481b-afba-3261811adf29",
   "metadata": {},
   "source": [
    "## The main difference between the Euclidean distance metric and the Manhattan distance metric in KNN is the way they treat large differences in feature values. The Euclidean distance metric squares the difference between each feature value, while the Manhattan distance metric does not. This means that the Euclidean distance metric is more sensitive to outliers, while the Manhattan distance metric is less sensitive to outliers.\n",
    "\n",
    "## This difference can affect the performance of a KNN classifier or regressor in a few ways. First, if the dataset contains outliers, the Euclidean distance metric may be more likely to identify the outliers as the nearest neighbors of a given data point. This can lead to inaccurate predictions, especially for classification tasks. The Manhattan distance metric is less likely to be affected by outliers, so it may perform better in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441af7e0-66b3-4bf3-bbc2-12e9915c9efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4897dbf6-53c7-4af3-9f31-826e2c19b00a",
   "metadata": {},
   "source": [
    "## Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60626e2f-4d8c-487d-8110-639916cb2cd4",
   "metadata": {},
   "source": [
    "## The best value of k will depend on the specific characteristics of your dataset and your task. However, there are a few techniques that can be used to determine the optimal k value:\n",
    "## Use a validation set.\n",
    "## Use k-fold cross-validation.\n",
    "## Use a grid search.\n",
    "## It is important to note that the optimal value of k may vary depending on the dataset and the task. Therefore, it is important to evaluate the performance of the KNN model for different values of k before deploying the model to production.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61bf73b-5627-46bd-a3be-bea8055f1da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e52024e-743c-4b8b-ba32-1a46b920ddb6",
   "metadata": {},
   "source": [
    "## Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728ab14-1c48-4941-850c-458ca612a6dd",
   "metadata": {},
   "source": [
    "## The choice of distance metric can affect the performance of a KNN classifier or regressor in a few ways:\n",
    "## Sensitivity to outliers.\n",
    "## Shape of the decision boundary.\n",
    "## Computational complexity.\n",
    "## In general, the Manhattan distance metric is a good choice for high-dimensional datasets and datasets that contain outliers. The Euclidean distance metric is a good choice for datasets that do not contain outliers and for datasets where a smooth decision boundary is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42d1a9-0a4a-44ca-9a96-eb54b9164c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bdaf815-e0da-4d65-9d52-574ca0a27f00",
   "metadata": {},
   "source": [
    "## Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6cdbd2-3a28-4397-bf5f-5263ecea9d37",
   "metadata": {},
   "source": [
    "## Common hyperparameters in KNN classifiers and regressors:\n",
    "## Number of neighbors (k).\n",
    "## Distance metric.\n",
    "##  Weights.\n",
    "\n",
    "## There are a number of ways to tune the hyperparameters of a KNN model to improve model performance. One common approach is to use a grid search. A grid search is a systematic approach to trying different combinations of hyperparameter values. To use a grid search to tune the hyperparameters of a KNN model, you would first specify a range of values for each hyperparameter. Then, you would train the KNN model for each combination of hyperparameter values and evaluate its performance on a validation set. The combination of hyperparameter values that produces the best performance on the validation set is likely to be the best combination of hyperparameter values for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac82e612-f605-4819-953f-aa95e97450a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac6a905-fa7e-46dd-a6ae-35a0434a824f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
